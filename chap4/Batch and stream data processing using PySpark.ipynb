{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea848f65-1df5-4ca5-a711-eaf6d0082cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005adc5d-0687-4a8b-864d-f32b8be51c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/16 23:45:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"chap4\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af6f67-56e5-468e-a6c4-213a9ad51eb8",
   "metadata": {},
   "source": [
    "# Batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b86a4-41ed-42ca-b905-2468033dd143",
   "metadata": {},
   "source": [
    "Generate some fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55df1a5-f053-4160-b6e3-0e3e185057af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "Faker.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c29ab8-3454-4145-a857-cb826c9fa87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(nrow):\n",
    "    data = [\n",
    "        (fake.unique.name(), fake.random_int(18, 25), fake.job())\n",
    "        for i in range(nrow)\n",
    "    ]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e91258-5b49-4b27-9ada-1c79bf1b384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+--------------------+\n",
      "|               name|age|                 job|\n",
      "+-------------------+---+--------------------+\n",
      "|       Norma Fisher| 20|Sales promotion a...|\n",
      "|Dr. Ronald Faulkner| 20|Television produc...|\n",
      "|     Colleen Taylor| 19|      Chief of Staff|\n",
      "|  Danielle Browning| 19|Insurance claims ...|\n",
      "| Benjamin Jefferson| 20|Public house manager|\n",
      "|    Heather Stewart| 19|                 Sub|\n",
      "|         Sean Green| 18|Chief Financial O...|\n",
      "|   Jennifer Summers| 18|  Veterinary surgeon|\n",
      "|    Sean Sanchez MD| 18|Engineer, aeronau...|\n",
      "|       Connie Pratt| 19|Speech and langua...|\n",
      "|       Bobby Flores| 21|Clinical embryolo...|\n",
      "|     Eddie Martinez| 20|Sound technician,...|\n",
      "|       Robert Payne| 20|Producer, televis...|\n",
      "|     Robert Stewart| 19|Horticultural the...|\n",
      "|    Roberto Johnson| 20|     Publishing copy|\n",
      "|   Michael Anderson| 19|     Arboriculturist|\n",
      "|  Stephanie Leblanc| 21|Scientist, water ...|\n",
      "|    Robert Atkinson| 21|        TEFL teacher|\n",
      "| Johnathan Davidson| 20|  Charity fundraiser|\n",
      "|     Brian Matthews| 20|      Energy manager|\n",
      "+-------------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(generate_data(50),\n",
    "                            [\"name\", \"age\", \"job\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368b86e-b4f1-4caf-9767-4e73c8ec0a09",
   "metadata": {},
   "source": [
    "Partition the data by a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7fb043-5a5a-4225-96da-bfd3e5065bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"fake_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb72aeea-6fa6-457f-b5f3-0437b5926780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove directory associated with the table (if it already exists)\n",
    "%rm -rf spark-warehouse/\"$db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2110e1-d72f-4ce8-95dc-98f6b35c5d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.write.saveAsTable(db, partitionBy=\"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0392b0f1-0f2f-4fb2-8314-38d0c83e9114",
   "metadata": {},
   "source": [
    "## Data Skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0f7d4-3432-4cdf-807d-d0f16aa9fa53",
   "metadata": {},
   "source": [
    "Data skew is when data is unevenly distributed across partitions. This slows down performance and needs handling. Most of the time, Spark's Adaptive Query Engine (AQE) is efficient in optimizing the data distribution. However, sometimes we need to manually fix the data skew problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d8849-4ef4-4edc-98c2-acfa5eea798a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Further reading**:\n",
    "\n",
    "- [Deep Dive into Handling Apache Spark Data Skew](https://chengzhizhao.com/deep-dive-into-handling-apache-spark-data-skew/) (Zhao, 2022)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
