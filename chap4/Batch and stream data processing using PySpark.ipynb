{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea848f65-1df5-4ca5-a711-eaf6d0082cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from faker import Faker\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    broadcast, spark_partition_id, rand\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005adc5d-0687-4a8b-864d-f32b8be51c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/17 23:20:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"chap4\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af6f67-56e5-468e-a6c4-213a9ad51eb8",
   "metadata": {},
   "source": [
    "# Batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b86a4-41ed-42ca-b905-2468033dd143",
   "metadata": {},
   "source": [
    "Generate some fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55df1a5-f053-4160-b6e3-0e3e185057af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "Faker.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfa1217-c7a1-45f0-9253-3e2d432c7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (fake.unique.name(), fake.random_int(18, 25), fake.job())\n",
    "    for _ in range(1000)   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e91258-5b49-4b27-9ada-1c79bf1b384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+--------------------+\n",
      "|               name|age|                 job|\n",
      "+-------------------+---+--------------------+\n",
      "|       Norma Fisher| 22|Sales promotion a...|\n",
      "|Dr. Ronald Faulkner| 23|Television produc...|\n",
      "|     Colleen Taylor| 20|      Chief of Staff|\n",
      "|  Danielle Browning| 20|Insurance claims ...|\n",
      "| Benjamin Jefferson| 23|Public house manager|\n",
      "|    Heather Stewart| 21|                 Sub|\n",
      "|         Sean Green| 18|Chief Financial O...|\n",
      "|   Jennifer Summers| 18|  Veterinary surgeon|\n",
      "|    Sean Sanchez MD| 19|Engineer, aeronau...|\n",
      "|       Connie Pratt| 20|Speech and langua...|\n",
      "|       Bobby Flores| 25|Clinical embryolo...|\n",
      "|     Eddie Martinez| 23|Sound technician,...|\n",
      "|       Robert Payne| 22|Producer, televis...|\n",
      "|     Robert Stewart| 21|Horticultural the...|\n",
      "|    Roberto Johnson| 22|     Publishing copy|\n",
      "|   Michael Anderson| 20|     Arboriculturist|\n",
      "|  Stephanie Leblanc| 24|Scientist, water ...|\n",
      "|    Robert Atkinson| 24|        TEFL teacher|\n",
      "| Johnathan Davidson| 23|  Charity fundraiser|\n",
      "|     Brian Matthews| 23|      Energy manager|\n",
      "+-------------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(data, [\"name\", \"age\", \"job\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368b86e-b4f1-4caf-9767-4e73c8ec0a09",
   "metadata": {},
   "source": [
    "Partition the data by a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7fb043-5a5a-4225-96da-bfd3e5065bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"fake_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb72aeea-6fa6-457f-b5f3-0437b5926780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove directory associated with the table (if it already exists)\n",
    "%rm -rf spark-warehouse/\"$db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d2110e1-d72f-4ce8-95dc-98f6b35c5d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.write.saveAsTable(db, partitionBy=\"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0392b0f1-0f2f-4fb2-8314-38d0c83e9114",
   "metadata": {},
   "source": [
    "## Data Skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0f7d4-3432-4cdf-807d-d0f16aa9fa53",
   "metadata": {},
   "source": [
    "Data skew is when data is unevenly distributed across partitions. This slows down performance and needs handling. Most of the time, Spark's Adaptive Query Engine (AQE) is efficient in optimizing the data distribution. However, sometimes we need to manually fix the data skew problem. Here are some ways to do it:\n",
    "- Configuring the number of partitions to use when shuffling data for joins or aggregations (i.e., the `spark.sql.shuffle.partitions` option). See the [Working With Partitions](http://localhost:8888/notebooks/chap3/Apache%20Spark%20deep%20dive.ipynb#Working-With-Partitions) section in the Chapter 3 notebook for more information.\n",
    "- Broadcast join: Send the smaller dataset across all nodes and then join each node's portion of the larger dataset. This is suitable for small-to-medium-sized DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9804d95e-5fad-4e9b-a50e-964586b4a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd.DataFrame({\n",
    "    \"name\": fake.random_sample([tup[0] for tup in data], 5),\n",
    "    \"catchPhrase\": [fake.unique.catch_phrase() for _ in range(5)]\n",
    "})\n",
    "df2 = spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14859c3b-c2de-4753-a0aa-8c2e893fcf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+--------------------+--------------------+\n",
      "|            name|age|                 job|         catchPhrase|\n",
      "+----------------+---+--------------------+--------------------+\n",
      "|     Daniel Cruz| 19|Nurse, mental health|Public-key mobile...|\n",
      "|Reginald Garrett| 24|     Arboriculturist|Visionary systema...|\n",
      "|  Victoria Reese| 25|Health and safety...|Multi-layered hyb...|\n",
      "| Brent Willis MD| 23|           Ecologist|Fundamental inter...|\n",
      "| Dustin Mcdowell| 22|Senior tax profes...|Multi-lateral zer...|\n",
      "+----------------+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(broadcast(df2), \"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b07d8d-1cd4-429b-affc-1ac0e259bfb3",
   "metadata": {},
   "source": [
    "- Salting (idea from cryptography): Add a random or unique identifier to each record. This is useful if we are unsure what column we want to repartition by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8800610-239c-4acb-93fc-fc0ebde99047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+--------------------+----+\n",
      "|               name|age|                 job|salt|\n",
      "+-------------------+---+--------------------+----+\n",
      "|       Norma Fisher| 22|Sales promotion a...|   7|\n",
      "|Dr. Ronald Faulkner| 23|Television produc...|   5|\n",
      "|     Colleen Taylor| 20|      Chief of Staff|   0|\n",
      "|  Danielle Browning| 20|Insurance claims ...|   3|\n",
      "| Benjamin Jefferson| 23|Public house manager|   7|\n",
      "|    Heather Stewart| 21|                 Sub|   2|\n",
      "|         Sean Green| 18|Chief Financial O...|   2|\n",
      "|   Jennifer Summers| 18|  Veterinary surgeon|   5|\n",
      "|    Sean Sanchez MD| 19|Engineer, aeronau...|   7|\n",
      "|       Connie Pratt| 20|Speech and langua...|   0|\n",
      "|       Bobby Flores| 25|Clinical embryolo...|   2|\n",
      "|     Eddie Martinez| 23|Sound technician,...|   6|\n",
      "|       Robert Payne| 22|Producer, televis...|   4|\n",
      "|     Robert Stewart| 21|Horticultural the...|   5|\n",
      "|    Roberto Johnson| 22|     Publishing copy|   3|\n",
      "|   Michael Anderson| 20|     Arboriculturist|   2|\n",
      "|  Stephanie Leblanc| 24|Scientist, water ...|   0|\n",
      "|    Robert Atkinson| 24|        TEFL teacher|   4|\n",
      "| Johnathan Davidson| 23|  Charity fundraiser|   6|\n",
      "|     Brian Matthews| 23|      Energy manager|   2|\n",
      "+-------------------+---+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"salt\", (rand(0) * 10).cast(\"int\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98d8849-4ef4-4edc-98c2-acfa5eea798a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Further reading**:\n",
    "\n",
    "- [Deep Dive into Handling Apache Spark Data Skew](https://chengzhizhao.com/deep-dive-into-handling-apache-spark-data-skew/) (Zhao, 2022)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
