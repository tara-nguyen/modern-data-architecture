{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a696bc7-ab9f-4ff9-af57-992aad0a477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24605a6d-b77e-4716-af33-ba05a34b2fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/24 23:56:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"chap3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6139a-be10-4eb3-af7f-167c6c27599e",
   "metadata": {},
   "source": [
    "# Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7acb442-1322-452e-943b-0053c413d28c",
   "metadata": {},
   "source": [
    "## Working With Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff233a5c-6cd8-4294-a3b1-584ec1a3b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516792ae-cd61-451e-9abe-075815404c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0, x=49, y=47), Row(id=1, x=53, y=-45), Row(id=2, x=33, y=15)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(i, random.randrange(100), random.randint(-50, 50))\n",
    "        for i in range(20)]\n",
    "df1 = spark.createDataFrame(data, schema=[\"id\", \"x\", \"y\"])\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e5376-0197-44ca-bbf5-15b376723020",
   "metadata": {},
   "source": [
    "Number of partitions in resilient distributed dataset (RDD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2648ef82-f83f-4a09-9a1a-0eb073a92ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285fbd10-63d7-4790-abd6-5e7401a4f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_partitions(df):\n",
    "    partitions = df.rdd.glom().collect()\n",
    "    for i in range(len(partitions)):\n",
    "        print(f\"Partition #{i + 1}\")\n",
    "        print(*partitions[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f5880c-5a3e-4df1-ae40-4383a3bbed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition #1\n",
      "Row(id=0, x=49, y=47) Row(id=1, x=53, y=-45)\n",
      "\n",
      "Partition #2\n",
      "Row(id=2, x=33, y=15) Row(id=3, x=62, y=1)\n",
      "\n",
      "Partition #3\n",
      "Row(id=4, x=38, y=11) Row(id=5, x=45, y=24)\n",
      "\n",
      "Partition #4\n",
      "Row(id=6, x=27, y=14) Row(id=7, x=17, y=-14) Row(id=8, x=17, y=46) Row(id=9, x=12, y=29)\n",
      "\n",
      "Partition #5\n",
      "Row(id=10, x=32, y=18) Row(id=11, x=90, y=27)\n",
      "\n",
      "Partition #6\n",
      "Row(id=12, x=18, y=-11) Row(id=13, x=12, y=43)\n",
      "\n",
      "Partition #7\n",
      "Row(id=14, x=9, y=37) Row(id=15, x=42, y=10)\n",
      "\n",
      "Partition #8\n",
      "Row(id=16, x=71, y=-38) Row(id=17, x=45, y=5) Row(id=18, x=40, y=28) Row(id=19, x=81, y=-24)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c4ba6-e0cd-47ee-8221-5807dfbe2cfb",
   "metadata": {},
   "source": [
    "Repartition without a shuffle (often used when reducing partitioning, i.e., decreasing the number of partitions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1651b61b-3d60-4867-855f-56d9036893a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition #1\n",
      "Row(id=0, x=49, y=47) Row(id=1, x=53, y=-45)\n",
      "\n",
      "Partition #2\n",
      "Row(id=2, x=33, y=15) Row(id=3, x=62, y=1) Row(id=4, x=38, y=11) Row(id=5, x=45, y=24)\n",
      "\n",
      "Partition #3\n",
      "Row(id=6, x=27, y=14) Row(id=7, x=17, y=-14) Row(id=8, x=17, y=46) Row(id=9, x=12, y=29)\n",
      "\n",
      "Partition #4\n",
      "Row(id=10, x=32, y=18) Row(id=11, x=90, y=27) Row(id=12, x=18, y=-11) Row(id=13, x=12, y=43)\n",
      "\n",
      "Partition #5\n",
      "Row(id=14, x=9, y=37) Row(id=15, x=42, y=10) Row(id=16, x=71, y=-38) Row(id=17, x=45, y=5) Row(id=18, x=40, y=28) Row(id=19, x=81, y=-24)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1a = df1.coalesce(5)\n",
    "print_partitions(rdd1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca18f0-9c9c-44a2-8e85-25cff79b652c",
   "metadata": {},
   "source": [
    "Repartition with a shuffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7326e6c-0216-4dd3-b279-9f3fd56258b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition #1\n",
      "Row(id=0, x=49, y=47) Row(id=2, x=33, y=15) Row(id=5, x=45, y=24) Row(id=7, x=17, y=-14) Row(id=11, x=90, y=27)\n",
      "\n",
      "Partition #2\n",
      "Row(id=8, x=17, y=46) Row(id=12, x=18, y=-11) Row(id=14, x=9, y=37) Row(id=19, x=81, y=-24)\n",
      "\n",
      "Partition #3\n",
      "Row(id=13, x=12, y=43) Row(id=15, x=42, y=10) Row(id=16, x=71, y=-38)\n",
      "\n",
      "Partition #4\n",
      "Row(id=6, x=27, y=14) Row(id=18, x=40, y=28)\n",
      "\n",
      "Partition #5\n",
      "Row(id=1, x=53, y=-45) Row(id=3, x=62, y=1) Row(id=4, x=38, y=11) Row(id=9, x=12, y=29) Row(id=10, x=32, y=18) Row(id=17, x=45, y=5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1b = df1.repartition(5)\n",
    "print_partitions(rdd1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3378e-ca49-4108-b40e-c0349ae90e86",
   "metadata": {},
   "source": [
    "Repartition by columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f17a07b9-7395-4b32-a3b7-52b3dc640813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition #1\n",
      "Row(id=0, x=49, y=47) Row(id=13, x=12, y=43) Row(id=14, x=9, y=37) Row(id=16, x=71, y=-38)\n",
      "\n",
      "Partition #2\n",
      "Row(id=3, x=62, y=1)\n",
      "\n",
      "Partition #3\n",
      "Row(id=4, x=38, y=11) Row(id=11, x=90, y=27) Row(id=15, x=42, y=10)\n",
      "\n",
      "Partition #4\n",
      "Row(id=2, x=33, y=15) Row(id=6, x=27, y=14) Row(id=7, x=17, y=-14) Row(id=8, x=17, y=46) Row(id=10, x=32, y=18) Row(id=12, x=18, y=-11) Row(id=17, x=45, y=5) Row(id=19, x=81, y=-24)\n",
      "\n",
      "Partition #5\n",
      "Row(id=1, x=53, y=-45) Row(id=5, x=45, y=24) Row(id=9, x=12, y=29) Row(id=18, x=40, y=28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df1.repartition(5, \"x\", \"y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341b522-e916-4428-8d03-eba67cb6e626",
   "metadata": {},
   "source": [
    "Set the default number of partitions to use when shuffling data (200 by default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06df46f-2077-4cf8-8352-f43c849f6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14f6aa-aaab-478e-954c-69ce944237ba",
   "metadata": {},
   "source": [
    "With Adaptive Query Engine (AQE), we don't really need to set this number since AQE automatically uses runtime statistics to optimize our Spark instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741951c-cebd-4c26-902a-7aa2f1c2d3e5",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c94737-11f3-4e62-801a-31ad8de21c4b",
   "metadata": {},
   "source": [
    "Cache a DataFrame with `cache()`: The default storage level is MEMORY_AND_DISK; data is stored in memory when using memory and disk modes, but if needed, disk mode is also used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88685dae-139e-4a54-b3ce-79c0abe7c3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, x: bigint, y: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4629371-e2cc-4d92-b1b4-dcb8dcb2e2ab",
   "metadata": {},
   "source": [
    "Validate that the data has been cached and stored correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e9e2897-cdc8-4ab0-8bb2-5cdf16c71595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c652c06-bf25-4d4e-94df-1d784fb5381d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.storageLevel.useMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e023963-fdc3-4d6c-acfb-6f17b8e9a986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.storageLevel.useDisk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f18088-3a75-433d-8cac-946c71011913",
   "metadata": {},
   "source": [
    "Remove data from cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e2586af-bea8-4072-be41-c602e0051294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.unpersist()\n",
    "df1.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8d223-af06-4c28-b6d3-5e357e5f7e23",
   "metadata": {},
   "source": [
    "Specify a storage level when caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff14966e-9682-470e-b7b4-cad166c5778c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.persist(StorageLevel.MEMORY_ONLY)\n",
    "df1.storageLevel.useMemory, df1.storageLevel.useDisk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e8d0c-eab1-4cd9-bd17-83e7a9fcff51",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6da1c0b-bc5a-4484-be64-9729defcedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(\n",
    "    [(1, \"a\"), (3, \"b\"), (5, \"c\")],\n",
    "    [\"id\", \"z\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e174c93-5da2-4f86-ab61-1c533fd7d5a9",
   "metadata": {},
   "source": [
    "Broadcast join: Send the smaller dataset across all nodes and then join each node's portion of the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4f07427-8243-4903-83f8-7be7a700d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "| id|  x|  y| id|  z|\n",
      "+---+---+---+---+---+\n",
      "|  1| 53|-45|  1|  a|\n",
      "|  3| 62|  1|  3|  b|\n",
      "|  5| 45| 24|  5|  c|\n",
      "+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(broadcast(df2), df1.id == df2.id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f63fba-5dcb-4c2d-9aa1-db648783a7f5",
   "metadata": {},
   "source": [
    "# Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c4f00-5067-4460-8f83-8fbb768057a8",
   "metadata": {},
   "source": [
    "## Grouping Tables With Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7672b64d-35c1-4e5a-8ee6-14b3dd01b3e2",
   "metadata": {},
   "source": [
    "Create a database, using the default location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40e65b1a-3a68-4bd4-86d6-7e295d6e318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"simple_database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc81ee2d-3472-43e4-a939-5aa665b77e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {db}\n",
    "    COMMENT 'Create a managed database'\"\"\"\n",
    ")\n",
    "db_info = spark.sql(f\"DESCRIBE DATABASE {db}\")\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {db} CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a59b167d-ec67-47ea-a871-b847b71885a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info_name', 'info_value']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "856e6e32-7529-4dff-81fd-cee4702af8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------+\n",
      "|info_name     |info_value               |\n",
      "+--------------+-------------------------+\n",
      "|Catalog Name  |spark_catalog            |\n",
      "|Namespace Name|simple_database          |\n",
      "|Comment       |Create a managed database|\n",
      "|Location      |[REDACTED]               |\n",
      "|Owner         |[REDACTED]               |\n",
      "+--------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db_info.withColumn(\n",
    "    \"info_value\",\n",
    "    when(\n",
    "        db_info[\"info_name\"].isin(\"Location\", \"Owner\"),\n",
    "        \"[REDACTED]\"\n",
    "    ).otherwise(db_info[\"info_value\"])\n",
    ").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
