{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a696bc7-ab9f-4ff9-af57-992aad0a477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24605a6d-b77e-4716-af33-ba05a34b2fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/13 17:47:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"chap3\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d22535-4b84-4203-9e12-b3a974e73148",
   "metadata": {},
   "source": [
    "NOTE: Some sections in the book chapter require the use of Databricks. For those sections, only code that does <u>not</u> involve Databricks will be presented in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6139a-be10-4eb3-af7f-167c6c27599e",
   "metadata": {},
   "source": [
    "# Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7acb442-1322-452e-943b-0053c413d28c",
   "metadata": {},
   "source": [
    "## Working With Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff233a5c-6cd8-4294-a3b1-584ec1a3b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516792ae-cd61-451e-9abe-075815404c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0, x=49, y=47), Row(id=1, x=53, y=-45), Row(id=2, x=33, y=15)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(i, random.randrange(100), random.randint(-50, 50))\n",
    "        for i in range(20)]\n",
    "df1 = spark.createDataFrame(data, schema=[\"id\", \"x\", \"y\"])\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e5376-0197-44ca-bbf5-15b376723020",
   "metadata": {},
   "source": [
    "Number of partitions in resilient distributed dataset (RDD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2648ef82-f83f-4a09-9a1a-0eb073a92ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "285fbd10-63d7-4790-abd6-5e7401a4f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_partitions(df):\n",
    "    partitions = df.rdd.glom().collect()\n",
    "    for i in range(len(partitions)):\n",
    "        print(f\"Partition #{i + 1}\")\n",
    "        print(*partitions[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f5880c-5a3e-4df1-ae40-4383a3bbed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition #1\n",
      "Row(id=0, x=49, y=47) Row(id=1, x=53, y=-45)\n",
      "\n",
      "Partition #2\n",
      "Row(id=2, x=33, y=15) Row(id=3, x=62, y=1)\n",
      "\n",
      "Partition #3\n",
      "Row(id=4, x=38, y=11) Row(id=5, x=45, y=24)\n",
      "\n",
      "Partition #4\n",
      "Row(id=6, x=27, y=14) Row(id=7, x=17, y=-14) Row(id=8, x=17, y=46) Row(id=9, x=12, y=29)\n",
      "\n",
      "Partition #5\n",
      "Row(id=10, x=32, y=18) Row(id=11, x=90, y=27)\n",
      "\n",
      "Partition #6\n",
      "Row(id=12, x=18, y=-11) Row(id=13, x=12, y=43)\n",
      "\n",
      "Partition #7\n",
      "Row(id=14, x=9, y=37) Row(id=15, x=42, y=10)\n",
      "\n",
      "Partition #8\n",
      "Row(id=16, x=71, y=-38) Row(id=17, x=45, y=5) Row(id=18, x=40, y=28) Row(id=19, x=81, y=-24)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c4ba6-e0cd-47ee-8221-5807dfbe2cfb",
   "metadata": {},
   "source": [
    "Repartition without a shuffle (often used when reducing partitioning, i.e., decreasing the number of partitions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1651b61b-3d60-4867-855f-56d9036893a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition #1\n",
      "Row(id=0, x=49, y=47) Row(id=1, x=53, y=-45)\n",
      "\n",
      "Partition #2\n",
      "Row(id=2, x=33, y=15) Row(id=3, x=62, y=1) Row(id=4, x=38, y=11) Row(id=5, x=45, y=24)\n",
      "\n",
      "Partition #3\n",
      "Row(id=6, x=27, y=14) Row(id=7, x=17, y=-14) Row(id=8, x=17, y=46) Row(id=9, x=12, y=29)\n",
      "\n",
      "Partition #4\n",
      "Row(id=10, x=32, y=18) Row(id=11, x=90, y=27) Row(id=12, x=18, y=-11) Row(id=13, x=12, y=43)\n",
      "\n",
      "Partition #5\n",
      "Row(id=14, x=9, y=37) Row(id=15, x=42, y=10) Row(id=16, x=71, y=-38) Row(id=17, x=45, y=5) Row(id=18, x=40, y=28) Row(id=19, x=81, y=-24)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1a = df1.coalesce(5)\n",
    "print_partitions(rdd1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca18f0-9c9c-44a2-8e85-25cff79b652c",
   "metadata": {},
   "source": [
    "Repartition with a shuffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7326e6c-0216-4dd3-b279-9f3fd56258b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition #1\n",
      "Row(id=0, x=49, y=47) Row(id=2, x=33, y=15) Row(id=5, x=45, y=24) Row(id=7, x=17, y=-14) Row(id=11, x=90, y=27)\n",
      "\n",
      "Partition #2\n",
      "Row(id=8, x=17, y=46) Row(id=12, x=18, y=-11) Row(id=14, x=9, y=37) Row(id=19, x=81, y=-24)\n",
      "\n",
      "Partition #3\n",
      "Row(id=13, x=12, y=43) Row(id=15, x=42, y=10) Row(id=16, x=71, y=-38)\n",
      "\n",
      "Partition #4\n",
      "Row(id=6, x=27, y=14) Row(id=18, x=40, y=28)\n",
      "\n",
      "Partition #5\n",
      "Row(id=1, x=53, y=-45) Row(id=3, x=62, y=1) Row(id=4, x=38, y=11) Row(id=9, x=12, y=29) Row(id=10, x=32, y=18) Row(id=17, x=45, y=5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1b = df1.repartition(5)\n",
    "print_partitions(rdd1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3378e-ca49-4108-b40e-c0349ae90e86",
   "metadata": {},
   "source": [
    "Repartition by columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f17a07b9-7395-4b32-a3b7-52b3dc640813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition #1\n",
      "Row(id=0, x=49, y=47) Row(id=13, x=12, y=43) Row(id=14, x=9, y=37) Row(id=16, x=71, y=-38)\n",
      "\n",
      "Partition #2\n",
      "Row(id=3, x=62, y=1)\n",
      "\n",
      "Partition #3\n",
      "Row(id=4, x=38, y=11) Row(id=11, x=90, y=27) Row(id=15, x=42, y=10)\n",
      "\n",
      "Partition #4\n",
      "Row(id=2, x=33, y=15) Row(id=6, x=27, y=14) Row(id=7, x=17, y=-14) Row(id=8, x=17, y=46) Row(id=10, x=32, y=18) Row(id=12, x=18, y=-11) Row(id=17, x=45, y=5) Row(id=19, x=81, y=-24)\n",
      "\n",
      "Partition #5\n",
      "Row(id=1, x=53, y=-45) Row(id=5, x=45, y=24) Row(id=9, x=12, y=29) Row(id=18, x=40, y=28)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df1.repartition(5, \"x\", \"y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341b522-e916-4428-8d03-eba67cb6e626",
   "metadata": {},
   "source": [
    "Set the default number of partitions to use when shuffling data (200 by default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06df46f-2077-4cf8-8352-f43c849f6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14f6aa-aaab-478e-954c-69ce944237ba",
   "metadata": {},
   "source": [
    "With Adaptive Query Engine (AQE), we don't really need to set this number since AQE automatically uses runtime statistics to optimize our Spark instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741951c-cebd-4c26-902a-7aa2f1c2d3e5",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c94737-11f3-4e62-801a-31ad8de21c4b",
   "metadata": {},
   "source": [
    "Cache a DataFrame with `cache()`: The default storage level is MEMORY_AND_DISK; data is stored in memory when using memory and disk modes, but if needed, disk mode is also used. It should be noted that retrieving data from memory is always significantly faster than on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88685dae-139e-4a54-b3ce-79c0abe7c3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, x: bigint, y: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4629371-e2cc-4d92-b1b4-dcb8dcb2e2ab",
   "metadata": {},
   "source": [
    "Validate that the data has been cached and stored correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e9e2897-cdc8-4ab0-8bb2-5cdf16c71595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c652c06-bf25-4d4e-94df-1d784fb5381d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.storageLevel.useMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e023963-fdc3-4d6c-acfb-6f17b8e9a986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.storageLevel.useDisk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f18088-3a75-433d-8cac-946c71011913",
   "metadata": {},
   "source": [
    "Remove data from cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e2586af-bea8-4072-be41-c602e0051294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.unpersist()\n",
    "df1.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8d223-af06-4c28-b6d3-5e357e5f7e23",
   "metadata": {},
   "source": [
    "Specify a storage level when caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff14966e-9682-470e-b7b4-cad166c5778c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.persist(StorageLevel.MEMORY_ONLY)\n",
    "df1.storageLevel.useMemory, df1.storageLevel.useDisk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e8d0c-eab1-4cd9-bd17-83e7a9fcff51",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6da1c0b-bc5a-4484-be64-9729defcedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(\n",
    "    [(1, \"a\"), (3, \"b\"), (5, \"c\")],\n",
    "    [\"id\", \"z\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e174c93-5da2-4f86-ab61-1c533fd7d5a9",
   "metadata": {},
   "source": [
    "Broadcast join: Send the smaller dataset across all nodes and then join each node's portion of the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4f07427-8243-4903-83f8-7be7a700d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "| id|  x|  y| id|  z|\n",
      "+---+---+---+---+---+\n",
      "|  1| 53|-45|  1|  a|\n",
      "|  3| 62|  1|  3|  b|\n",
      "|  5| 45| 24|  5|  c|\n",
      "+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(broadcast(df2), df1.id == df2.id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f63fba-5dcb-4c2d-9aa1-db648783a7f5",
   "metadata": {},
   "source": [
    "# Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c4f00-5067-4460-8f83-8fbb768057a8",
   "metadata": {},
   "source": [
    "## Grouping Tables With Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7672b64d-35c1-4e5a-8ee6-14b3dd01b3e2",
   "metadata": {},
   "source": [
    "Create a database, using the default location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40e65b1a-3a68-4bd4-86d6-7e295d6e318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"simple_database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc81ee2d-3472-43e4-a939-5aa665b77e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {db} CASCADE\")\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE DATABASE {db}\n",
    "    COMMENT 'Create a managed database'\"\"\"\n",
    ")\n",
    "db_info = spark.sql(f\"DESCRIBE DATABASE {db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a59b167d-ec67-47ea-a871-b847b71885a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['info_name', 'info_value']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "856e6e32-7529-4dff-81fd-cee4702af8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------------+\n",
      "|info_name     |info_value               |\n",
      "+--------------+-------------------------+\n",
      "|Catalog Name  |spark_catalog            |\n",
      "|Namespace Name|simple_database          |\n",
      "|Comment       |Create a managed database|\n",
      "|Location      |[REDACTED]               |\n",
      "|Owner         |[REDACTED]               |\n",
      "+--------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db_info.withColumn(\n",
    "    \"info_value\",\n",
    "    when(\n",
    "        db_info[\"info_name\"].isin(\"Location\", \"Owner\"),\n",
    "        \"[REDACTED]\"\n",
    "    ).otherwise(db_info[\"info_value\"])\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee701f-6a48-402c-97e3-f1175c80581c",
   "metadata": {},
   "source": [
    "Check if the database exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "521d9110-a54a-4a75-8d99-dacb541e9020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.databaseExists(db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d472745a-c873-4ec2-9147-51740213eb43",
   "metadata": {},
   "source": [
    "Syntax for specifying database location:\n",
    "\n",
    "```sql\n",
    "CREATE DATABASE IF NOT EXISTS database_name\n",
    "LOCATION 'path/to/directory'\n",
    "```\n",
    "\n",
    "Current default database in this Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b58e643b-1377-485f-a0e6-8e712705c5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c59f0-fbb3-45f3-844a-e0858ed663b3",
   "metadata": {},
   "source": [
    "Set the current default database in this session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b07fb507-5958-4265-9f09-9bc8408d4ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simple_database'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.setCurrentDatabase(db)\n",
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb366d6-68dd-490c-b897-d1c22610205b",
   "metadata": {},
   "source": [
    "List of databases available across all sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09c27936-4236-488f-8236-5727d34cbad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='[REDACTED]'),\n",
       " Database(name='simple_database', catalog='spark_catalog', description='Create a managed database', locationUri='[REDACTED]')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_db = spark.catalog.listDatabases()\n",
    "all_db = [db._replace(locationUri=\"[REDACTED]\") for db in all_db]\n",
    "all_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ddda20-8e32-4fba-a9b0-133a53758ce6",
   "metadata": {},
   "source": [
    "## Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a37d21a-7147-4f41-85c2-fb7e5d9c3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Alex\", 20), (\"Eugene\", 27),\n",
    "        (\"Emily\", 23), (\"Mabel\", 23)\n",
    "    ],\n",
    "    [\"name\", \"age\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c769f-4dc5-492c-a47c-d56c6f04cf01",
   "metadata": {},
   "source": [
    "### Managed Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce0508-d7ce-4f1b-9728-4be9ebe7e9a7",
   "metadata": {},
   "source": [
    "Create a managed table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f877864e-b3a2-4715-b457-b41686e77bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_name = \"friend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e650370d-0696-4c4b-a5d7-ee8a603faa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove directory associated with the table (if it already exists)\n",
    "%rm -rf spark-warehouse/\"$db\".db/\"$table1_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0271c78-80ab-40e8-bd11-df5557883b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write.saveAsTable(table1_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a0f69-7dc6-44f0-bb0e-4ee30dface96",
   "metadata": {},
   "source": [
    "List all tables in the current database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "927449dc-f1fa-45e3-9802-ac6dae5f1573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='friend', catalog='spark_catalog', namespace=['simple_database'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8824c6-4fd7-4810-9991-af129a63b114",
   "metadata": {},
   "source": [
    "Summary stats and description of table columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afe63a96-17af-4f4f-8f92-bb9f0ca4fa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+\n",
      "|summary| name|               age|\n",
      "+-------+-----+------------------+\n",
      "|  count|    4|                 4|\n",
      "|   mean| null|             23.25|\n",
      "| stddev| null|2.8722813232690143|\n",
      "|    min| Alex|                20|\n",
      "|    max|Mabel|                27|\n",
      "+-------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e41f512-4a2e-4567-9bfa-c738f2627ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|    name|   string|   null|\n",
      "|     age|   bigint|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE TABLE {table1_name}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d849754-39a9-4eb3-9c12-48480ae46e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                name|              string|   null|\n",
      "|                 age|              bigint|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|     simple_database|       |\n",
      "|               Table|              friend|       |\n",
      "|        Created Time|Sat Jan 13 17:47:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.4.1|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/tara-...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {table1_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318b31c-25d4-45b4-8146-32b4d275a050",
   "metadata": {},
   "source": [
    "### Unmanaged Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f505bf-810f-4d11-956d-a35cc9820c54",
   "metadata": {},
   "source": [
    "Create an unmanaged/external table: Use `pyspark.sql.DataFrameWriter.option()` to define a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f01b7973-a08d-45ae-a1cb-dfaa841ff0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "table2_name = \"friend_external\"\n",
    "external = \"external\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7384a07-ef62-4f1f-9187-02d9d29bc55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='friend', catalog='spark_catalog', namespace=['simple_database'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='friend_external', catalog='spark_catalog', namespace=['simple_database'], description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df3.write\n",
    "        .option(\"path\", external)\n",
    "        .saveAsTable(table2_name, \"json\", \"overwrite\")\n",
    ")\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb69e6-6a26-4ea3-8293-b693b7300a6e",
   "metadata": {},
   "source": [
    "### Reading Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e5307-0f47-45cb-8347-0f5fbecff88d",
   "metadata": {},
   "source": [
    "Load tables from a data source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3999695c-576d-4531-82cf-f079c9ed8835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 27|Eugene|\n",
      "| 23| Emily|\n",
      "| 23| Mabel|\n",
      "| 20|  Alex|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.load(external, \"json\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c024eff-2885-4cf0-87a8-c91e1f122f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 27|Eugene|\n",
      "| 23| Emily|\n",
      "| 23| Mabel|\n",
      "| 20|  Alex|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(external).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f082e62-847e-4c38-bf4c-893d06600fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_path = f\"spark-warehouse/{db}.db/{table1_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3ce450d-b638-47f3-9c21-7724ca32968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Eugene| 27|\n",
      "| Mabel| 23|\n",
      "| Emily| 23|\n",
      "|  Alex| 20|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.load(table1_path).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "618b98b7-36e5-4ca1-9d32-cc041dbaf9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Eugene| 27|\n",
      "| Mabel| 23|\n",
      "| Emily| 23|\n",
      "|  Alex| 20|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(table1_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63faae-4d55-4fbf-a25c-2727aa50598d",
   "metadata": {},
   "source": [
    "Retrieve a managed table (not applicable to unmanaged tables unless they reside inside the database directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33978701-0ad5-40a0-befd-f94d69456784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Eugene| 27|\n",
      "| Mabel| 23|\n",
      "| Emily| 23|\n",
      "|  Alex| 20|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(table1_name).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf64e6-aee6-4101-bc0c-f62412ef4b40",
   "metadata": {},
   "source": [
    "# Practical Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9e9de-c4ef-4475-ab8c-e469140f42cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
